# ğŸš€ Prompt Optimization Toolkit

An interactive and visual Streamlit app that helps Prompt Engineers test, compare, and fine-tune prompt strategies across Large Language Models (LLMs) like LLaMA 3 and Mixtral using the **Groq API**.

---

## ğŸ¯ Project Objective

To build a **Prompt Tuning & Evaluation Dashboard** where users can:
- Select prompt styles (e.g., instructive, creative, interrogative)
- Compare responses from two LLMs side-by-side
- Track token usage and temperature effect
- Rank and store the best-performing prompts

---

## ğŸ“¸ Live Demo

> **Coming Soon** â€“ Link to Streamlit Cloud or local app instructions

---

## ğŸ§  Key Features

| Feature | Description |
|--------|-------------|
| ğŸ”„ Compare Two LLMs | View outputs from LLaMA 3 and Mixtral models side-by-side |
| âœï¸ Prompt Templates | Instructive, Story-style, Question-formatted prompts built-in |
| ğŸ“Š Metrics Display | Token usage, temperature, word count, relevance |
| ğŸ† Leaderboard (planned) | Save top-performing prompts based on results |
| ğŸ“ Modular Code | Organized with `utils.py`, `prompts.json`, and clean `app.py` |

---

## ğŸ§ª Tech Stack

- **Frontend:** Streamlit
- **Backend/API:** Groq LLMs (via OpenAI-compatible endpoint)
- **Languages:** Python
- **Libraries:** requests, json, Streamlit
- **Other Tools:** Git, VS Code

---

## âš™ï¸ How to Run Locally

1. **Clone this repo:**
   ```bash
   git clone https://github.com/VenkataDurgaPrasadX/prompt-optimization-toolkit.git
   cd prompt-optimization-toolkit
2. Install dependencies:
  - pip install streamlit requests


3. Start the app:
  - streamlit run app.py

4. Enter your Groq API key from the sidebar to get started.
 -    prompt-optimization-toolkit/
- â”œâ”€â”€ app.py                # Main Streamlit app
- â”œâ”€â”€ utils.py              # API call + token usage logic
- â”œâ”€â”€ prompts.json          # Predefined prompt styles
- â”œâ”€â”€ README.md             # Project description
- â””â”€â”€ requirements.txt      # (Optional) For deployment

**ğŸ’¡ Use Cases**
ğŸ”¬ Experimenting with prompt variations for real-world LLM performance

ğŸ§ª Training Prompt Engineers on prompt tuning workflows

ğŸ“ˆ Evaluating token cost/performance balance in AI products


**ğŸ“š Future Enhancements**
Export chat history as .json or .txt

Save & rank prompts to leaderboard

Add timestamps and model confidence scores

Dark mode and CSS chat bubble layout


**ğŸ§‘â€ğŸ’» Author**
Tattukolla Venkata Durga Prasad
ğŸ“§ venkatadurgaprasad04@gmail.com
ğŸ”— LinkedIn | GitHub


**ğŸ“œ License**
This project is licensed under the MIT License.



